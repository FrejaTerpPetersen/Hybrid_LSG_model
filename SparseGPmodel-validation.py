# -*- coding: utf-8 -*-
"""
Created on Tue Sep 28 14:05:53 2021

@author: nfraehr
"""

# %% Import libraries
import gpflow
import matplotlib
from matplotlib import pyplot as plt
from matplotlib.dates import DateFormatter
import matplotlib.patches as mpatches
from matplotlib.offsetbox import AnchoredText
from matplotlib_scalebar.scalebar import ScaleBar

from mikeio import Dfsu
import numpy as np
import pandas as pd
import time

from EOF_Functions import CreatePseudoECs, ReconstructDataFromEOFandECs
from Datamanagement_Functions import tuple2array

# %% Import data
data = np.load('Managed_Data/Events_data/Binary_Data_Val_Events.npz', allow_pickle=True)

HF_events_bin = tuple(data['HF_events_bin'])
LF_int_events_bin = tuple(data['LFint_events_bin'])
Time = tuple(data['Time'])
Area = data['Area']

#LF data for comparison
data_LF = np.load('Managed_Data/Events_data/Binary_Data_Val_Events_LFmodel.npz', allow_pickle=True)
LF_events_bin = tuple(data_LF['LF_events_bin'])
LF_Area = data_LF['Area']

val_events = [1, 3, 6] #
# %% Functions
# Load SPGP classification models
def load_sparse_gaussian_models(n_output_ecs=1):
    k_exp_list = []
    spgp_model_list = []
    scaler_x_list = []
    scaler_y_list = []

    for iECs in range(n_output_ecs):
        # Load data
        fullpath = 'Managed_Data/SPGP_class_models/SPGP_cls_trained_ECs_%s.npz' % iECs

        data = np.load(fullpath, allow_pickle=True)
        m_param = data['param'].item()
        x_train = data['x_train']
        y_train = data['y_train']
        n_inducing_points = data['M']
        scaler_x = data['scaler_x'][()]
        scaler_y = data['scaler_y'][()]

        # Scaler lists
        scaler_x_list.append(scaler_x)
        scaler_y_list.append(scaler_y)

        # Find initial lengthscale
        # ini_length = np.array([abs(x_train).max(axis=0)])
        ini_length = np.mean(abs(x_train))

        # Appending kernel to list
        # k_exp_list.append(gpflow.kernels.Exponential(variance=1, lengthscales=ini_length))
        k_exp_list.append(gpflow.kernels.Exponential(variance=1, lengthscales=ini_length))

        # Create inducing variables
        dim = x_train.shape[1]
        inducing_points = np.c_[np.linspace(x_train[:, 0].min(), x_train[:, 0].max(), n_inducing_points)]
        for j in range(1, dim):
            inducing_points = np.c_[inducing_points,
                                    np.linspace(x_train[:, j].min(), x_train[:, j].max(), n_inducing_points)]

        # create multi-output inducing variables from Z
        iv = gpflow.inducing_variables.InducingPoints(inducing_points)

        # Appending SPGP model to list
        spgp_model_list.append(gpflow.models.SGPR(data=(x_train, y_train),
                                                  kernel=k_exp_list[iECs], inducing_variable=iv))

        # Initialise model
        gpflow.utilities.multiple_assign(spgp_model_list[iECs], m_param)

    return spgp_model_list, scaler_x_list, scaler_y_list


# Predicting ECs using the SPGP models
def spgp_model_predict(lowfidelity_ecs_val, n_input_ecs=1, n_output_ecs=1):
    # Load model
    spgp_list, sc_x, sc_y = load_sparse_gaussian_models(n_output_ecs=n_output_ecs)

    # Validation data
    n_input_ecs = np.array(range(n_input_ecs))

    x_val = np.c_[lowfidelity_ecs_val[:, n_input_ecs]]

    for iECs in range(n_output_ecs):
        # Load model corresponding to the ECs
        spgp_model = spgp_list[iECs]

        # Scaling data
        x_val_sc = sc_x[iECs].transform(x_val)

        # Predictions
        y_pred_mean, y_pred_var = spgp_model.predict_y(x_val_sc)
        y_pred_mean = y_pred_mean.numpy()
        y_pred_var = y_pred_var.numpy()

        # Confidence interval
        y__pred_conf = 1.96 * np.sqrt(y_pred_var)
        y_lb = y_pred_mean - y__pred_conf
        y_ub = y_pred_mean + y__pred_conf

        # Scaled back to real values
        y_pred_mean = sc_y[iECs].inverse_transform(y_pred_mean)
        y_lb = sc_y[iECs].inverse_transform(y_lb)
        y_ub = sc_y[iECs].inverse_transform(y_ub)

        # Save predicted ECs to array
        if iECs == 0:
            ecs_pred = y_pred_mean
            ecs_pred_lb = y_lb
            ecs_pred_ub = y_ub
        else:
            ecs_pred = np.c_[ecs_pred, y_pred_mean]
            ecs_pred_lb = np.c_[ecs_pred_lb, y_lb]
            ecs_pred_ub = np.c_[ecs_pred_ub, y_ub]

    return ecs_pred, ecs_pred_lb, ecs_pred_ub


# Reconstruct all data
def reconstruct_binary_highres_data(highfidelity_val_data, eof_spatialpattern, highfidelity_mean_train,
                                    ecs_temporalfunction, floodplain_index, river_index, weights, dry_threshold=0.5):
    # Reconstruct floodplain data
    recon_floodplain = ReconstructDataFromEOFandECs(eof_spatialpattern[0:ecs_temporalfunction.shape[1], :],
                                                    ecs_temporalfunction, highfidelity_mean_train, weights)
    # convert to binary
    recon_floodplain_bin = np.where(recon_floodplain > dry_threshold, 1, 0)

    # Reconstruct all cells
    recon_bin_all = np.zeros(highfidelity_val_data.shape)
    recon_bin_all[:, floodplain_index] = recon_floodplain_bin
    recon_bin_all[:, river_index] = 1

    return recon_bin_all


# Convert validation data to tuple list for each validation event
def valdata2eventtuple(val_data_array, val_time):
    # Start timestep of first event
    event_start_timestep = 0
    for ival in range(len(val_time)):
        # Find end of event
        event_stop_timestep = event_start_timestep + len(val_time[ival])
        # Save events to tuple
        if ival == 0:
            val_data_events = (val_data_array[event_start_timestep:event_stop_timestep, :],)
        else:
            val_data_events = val_data_events + (val_data_array[event_start_timestep:event_stop_timestep, :],)

        # Find start of next event
        event_start_timestep = event_stop_timestep

    return val_data_events


# Calculate POD and RFA for one timestep
def pod_rfa_1tstep(true_labels, pred_labels):
    true_pos_all = np.sum(np.where((pred_labels > 0) & (true_labels > 0), 1, 0))
    true_neg_all = np.sum(np.where((pred_labels == 0) & (true_labels == 0), 1, 0))
    false_pos_all = np.sum(np.where((pred_labels > 0) & (true_labels == 0), 1, 0))
    false_neg_all = np.sum(np.where((pred_labels == 0) & (true_labels > 0), 1, 0))
    # ConfusionMatrix_sum = True_pos + True_neg + False_pos + False_neg

    precision_all = true_pos_all / (true_pos_all + false_pos_all)
    recall_all = true_pos_all / (true_pos_all + false_neg_all)
    fscore_all = (2 * precision_all * recall_all) / (precision_all + recall_all)

    # Rate of false alarm and probability of detection
    pod_all = recall_all
    rfa_all = (false_pos_all / (true_pos_all + false_pos_all))

    return pod_all, rfa_all


# Calculate POD and RFA for each timestep in array
def pod_fra_all_tstep(true_labels, pred_labels):
    tn = np.zeros(len(true_labels))
    fp = tn.copy()
    fn = tn.copy()
    tp = tn.copy()
    precision = tn.copy()
    recall = tn.copy()
    fscore = tn.copy()
    pod = tn.copy()
    rfa = tn.copy()

    for idx in range(len(true_labels)):
        tp[idx] = np.sum(np.where((pred_labels[idx, :] > 0) & (true_labels[idx, :] > 0), 1, 0))
        tn[idx] = np.sum(np.where((pred_labels[idx, :] == 0) & (true_labels[idx, :] == 0), 1, 0))
        fp[idx] = np.sum(np.where((pred_labels[idx, :] > 0) & (true_labels[idx, :] == 0), 1, 0))
        fn[idx] = np.sum(np.where((pred_labels[idx, :] == 0) & (true_labels[idx, :] > 0), 1, 0))

        precision[idx] = tp[idx] / (tp[idx] + fp[idx])
        recall[idx] = tp[idx] / (tp[idx] + fn[idx])
        fscore[idx] = (2 * precision[idx] * recall[idx]) / (precision[idx] + recall[idx])

        # Rate of false alarm and probability of detection
        pod[idx] = recall[idx]
        rfa[idx] = fp[idx] / (tp[idx] + fp[idx])

    return pod, rfa


# Function for determining if inundation is detected correctly or not.
def inundation_detection(true_labels, pred_labels):
    # find correctly detected
    detected_flood = np.where((pred_labels > 0) & (true_labels > 0))[0]
    detected_dry = np.where((pred_labels == 0) & (true_labels == 0))[0]

    # Find false alarm -> predicted flooded, but is not flooded
    false_alarm = np.where((pred_labels > 0) & (true_labels == 0))[0]
    # Find misses -> predicted dry, but is flooded
    misses = np.where((pred_labels == 0) & (true_labels > 0))[0]

    # Assemble data to matrix with all cells
    inundation_detection = np.zeros(len(true_labels))
    inundation_detection[detected_dry] = -1
    inundation_detection[detected_flood] = 0
    inundation_detection[false_alarm] = 1
    inundation_detection[misses] = 2

    return inundation_detection


# %% Import EOF data
EOF_data = np.load('Managed_Data/SPGP_class_models/EOF_data.npz', allow_pickle=True)

HF_EOFs = EOF_data['HF_EOFs']
HF_mean_train = EOF_data['HF_mean_train']
n_ECs = EOF_data['n_ECs']
Area_FP = EOF_data['Area_FP']

# %% Load category data
Cat_data = np.load('Managed_Data/SPGP_class_models/Categories_Training_data.npz',
                   allow_pickle=True)

AD_idx = Cat_data['AD_idx']  # Always dry
wet_idx = Cat_data['wet_idx']  # Wet cells (river+floodplain)
AF_idx = Cat_data['AF_idx']  # Always flooded
TF_idx = Cat_data['TF_idx']  # Temporary flooded

# %% Validation data
start_time = time.monotonic()

# Validation data
HF_val = tuple2array(HF_events_bin)
LF_int_val = tuple2array(LF_int_events_bin)

# Validation data for temporary flooded cells
HF_val_FP = HF_val[:, TF_idx]
LF_int_val_FP = LF_int_val[:, TF_idx]

HF_ECs_val = CreatePseudoECs(HF_val_FP, HF_EOFs[0:n_ECs, :], Area_FP)
LF_int_pECs_val = CreatePseudoECs(LF_int_val_FP, HF_EOFs[0:n_ECs, :], Area_FP)

Val_data_endtime = time.monotonic()
# %% Predictions
ECs_pred, ECs_pred_LB, ECs_pred_UB = spgp_model_predict(LF_int_pECs_val,
                                                        n_input_ecs=n_ECs,
                                                        n_output_ecs=n_ECs)
Prediction_endtime = time.monotonic()

# %% Reconstruct HF data
# Reconstruct LF converted data
LFint2HF_recon = reconstruct_binary_highres_data(HF_val, HF_EOFs, HF_mean_train, ECs_pred, TF_idx,
                                                 AF_idx, Area_FP, dry_threshold=0.5)

# Reconstruct using true ECs from the HF data for comparison
HF_recon = reconstruct_binary_highres_data(HF_val, HF_EOFs, HF_mean_train, HF_ECs_val[:, 0:n_ECs], TF_idx, AF_idx,
                                           Area_FP,
                                           dry_threshold=0.5)

# Divide validation data into each event
# Reconstructed data
LFint2HF_recon_events = valdata2eventtuple(LFint2HF_recon, Time)

HF_recon_events = valdata2eventtuple(HF_recon, Time)

Reconstruction_endtime = time.monotonic()

# %% Timing
with open('predictiontime.txt', 'w') as f:
    Validation_data_time = Val_data_endtime - start_time
    f.write('Time for generating validation data: %i s \n' % Validation_data_time)

    Prediction_time = Prediction_endtime - Val_data_endtime
    f.write('Prediction time for SPGP model: %i s \n' % Prediction_time)

    Reconstruction_time = Reconstruction_endtime - Prediction_endtime
    f.write('Reconstruction time: %i s \n' % Reconstruction_time)

    Total_time = Reconstruction_endtime - start_time
    f.write('Total time: %i s \n' % Total_time)

    f.close()

# %% Plot ECs
fig = plt.figure(figsize=(12, 10))
for i in range(1, 4):
    ax1 = plt.subplot(3, 1, i)
    k = i - 1
    ax1.title.set_text('Mode %i' % i)
    ax1.plot(HF_ECs_val[:, k], 'k-', label='High-fidelity ECs')
    ax1.plot(LF_int_pECs_val[:, k], 'r-', label='Low-fidelity ECs')

    ax1.plot(ECs_pred[:, k], 'b--', label='LSG model')
    ax1.fill_between(range(len(ECs_pred)), ECs_pred_LB[:, k], ECs_pred_UB[:, k],
                     label='GP conf. 95%', alpha=0.75, edgecolor='gray', facecolor='cyan')

    ax1.set_ylabel('ECs [-]')
    ax1.grid()

ax1.legend(title='Legend')

fig.tight_layout()

fig.savefig('Managed_Data/Classification_Figures/ECs_val_events.png',
            bbox_inches="tight", dpi=300)
plt.show()

# %% Calculate flooded area
RMSE = np.zeros(len(val_events)) #Root mean square error
Rel_RMSE = np.zeros(len(val_events)) #Relative RMSE
Rel_Peak_value_error = np.zeros(len(val_events)) #relative Peak Value Estimate
Rel_Peak_timing_error_peakperiod = np.zeros(len(val_events)) #relative average peak time error compared to the peak period
Rel_Peak_timing_error_floodperiod = np.zeros(len(val_events)) #relative average peak time error compared to the rising limb of the flood event

fig = plt.figure(figsize=(9, 6))
date_form = DateFormatter("%m-%Y")

for ival in range(len(val_events)):
    # True flooded area HF model
    HF_val_Aflood = np.sum(HF_events_bin[ival] * Area, axis=1)
    # LF without doing anything
    LF_Aflood_all = np.sum(LF_events_bin[ival] * LF_Area, axis=1)

    # HF data reconstruction using predicted LF ECs
    LFint2HF_recon_Aflood = np.sum(LFint2HF_recon_events[ival] * Area, axis=1)

    # Reconstructed using true ECs
    HF_recon_Aflood = np.sum(HF_recon_events[ival] * Area, axis=1)

    # Plot flooded area for all validation data
    index_subplot = ival + 1
    ax1 = plt.subplot(2, 2, index_subplot)
    ax1.title.set_text('Event %i'%val_events[ival])
    ax1.plot(Time[ival], HF_val_Aflood / 1e6, 'k-', label='High-fidelity simulation')
    ax1.plot(Time[ival], LF_Aflood_all / 1e6, 'r-', label='Low-fidelity simulation')
    ax1.plot(Time[ival], HF_recon_Aflood / 1e6, 'k--', label='Prediction with true ECs')

    ax1.plot(Time[ival], LFint2HF_recon_Aflood / 1e6, 'b-', linewidth=1.0, label='LSG simulation')

    ax1.set_ylabel('Inundation extent [km²]')
    ax1.grid()
    ax1.xaxis.set_major_formatter(date_form)
    for tick in ax1.get_xticklabels():
        tick.set_rotation(45)

    # Inundation extent - evaluation
    n_tsteps_event = len(HF_val_Aflood)  # Total timesteps in event
    n_tsteps_5pc = round(0.05 * n_tsteps_event)  # Time steps for 5 percent

    # RMSE
    RMSE[ival] = np.sqrt((np.square(LFint2HF_recon_Aflood - HF_val_Aflood)).mean(axis=0))
    Rel_RMSE[ival] = RMSE[ival] / (HF_val_Aflood.mean(axis=0))

    # Peak values
    Peak_tsteps_SF = np.sort(np.argsort(-LFint2HF_recon_Aflood)[0:n_tsteps_5pc])
    Peak_tsteps_HF = np.sort(np.argsort(-HF_val_Aflood)[0:n_tsteps_5pc])

    Rel_Peak_value_error[ival] = (LFint2HF_recon_Aflood[Peak_tsteps_SF].mean() - HF_val_Aflood[Peak_tsteps_HF].mean()) / \
                                 HF_val_Aflood[Peak_tsteps_HF].mean()

    tstep_min_10pc = np.min(np.where(HF_val_Aflood > np.min(HF_val_Aflood) * 1.1)[0])
    peak_period_HF = Peak_tsteps_HF.max() - Peak_tsteps_HF.min()
    Rel_Peak_timing_error_peakperiod[ival] = (Peak_tsteps_SF.mean() - Peak_tsteps_HF.mean()) / \
                                  (peak_period_HF)
    Rel_Peak_timing_error_floodperiod[ival] = (Peak_tsteps_SF.mean() - Peak_tsteps_HF.mean()) / \
                                  (Peak_tsteps_HF.mean() - tstep_min_10pc)


fig.tight_layout()
leg = ax1.legend(title='Legend', loc='center left', bbox_to_anchor=(1.35, 0.5), frameon=False)
leg._legend_box.align = "left"

fig.savefig('Managed_Data/Classification_Figures/Floodedarea_valevents.png',
            bbox_inches="tight", dpi=300)
plt.show()

# %%Precision, recall and F-score for whole area

# Create array where each row is an event and each columns is a cell
LFint2HF_rec_events_max = np.zeros([len(val_events), HF_val.shape[1]], dtype=int)

HF_val_events_max = LFint2HF_rec_events_max.copy()

# Array to store POD and RFA data
LFint2HF_POD = np.zeros(len(val_events))
LFint2HF_RFA = np.zeros(len(val_events))

# event_start_timestep = 0
for ival in range(len(val_events)):
    # Get maximum extend of event for reconstruction
    LFint2HF_rec_events_max[ival, :] = LFint2HF_recon_events[ival].max(axis=0)

    # Get maximum extend of event
    HF_val_events_max[ival, :] = HF_events_bin[ival].max(axis=0)

    # POD and RFA
    LFint2HF_POD[ival], LFint2HF_RFA[ival] = pod_rfa_1tstep(HF_val_events_max[ival, :],
                                                            LFint2HF_rec_events_max[ival, :])

# %% Check POD and RFA for each timestep
POD_min = 1
RFA_max = 0

fig = plt.figure(figsize=(9, 6))

for ival in range(len(val_events)):
    LFint2HF_POD_alltstp, LFint2HF_RFA_alltstp = pod_fra_all_tstep(HF_events_bin[ival],
                                                                   LFint2HF_recon_events[ival])
    # Find lowest POD value and highest RFA
    if LFint2HF_POD_alltstp.min() < POD_min:
        POD_min = LFint2HF_POD_alltstp.min()

    if LFint2HF_RFA_alltstp.max() > RFA_max:
        RFA_max = LFint2HF_RFA_alltstp.max()

    index_subplot = ival + 1
    ax1 = plt.subplot(2, 2, index_subplot)
    ax1.title.set_text('Event %i' % val_events[ival])
    ax1.plot(Time[ival], LFint2HF_POD_alltstp, 'b-', label='Probability of detection')

    ax1.set_ylim([0, 1])
    ax1.set_ylabel('Detection')

    ax1.plot(Time[ival], LFint2HF_RFA_alltstp, 'r--', label='Rate of false alarm')

    ax1.grid()
    ax1.xaxis.set_major_formatter(date_form)
    for tick in ax1.get_xticklabels():
        tick.set_rotation(45)

fig.tight_layout()
leg = ax1.legend(title='Legend', loc='center left', bbox_to_anchor=(1.35, 0.5), frameon=False)
leg._legend_box.align = "left"

fig.savefig('Managed_Data/Classification_Figures/POD_RFA_valevents.png',
            bbox_inches="tight", dpi=300)
plt.show()
# %% Examining location of highest errors

# Load Mike21 data for plotting
dfs_HF = Dfsu("Raw_Data/HF/Chow_HF_20110701_20111015.dfsu")  # dfsu object


color_map = matplotlib.colors.ListedColormap(['dodgerblue', 'yellow', 'red'])
cm = 1/2.54
fig = plt.figure(figsize=(15*cm, 12*cm))

for ival in range(len(val_events)):
    # Find locations where model differs from HF model
    LFint2HF_InunDetect = inundation_detection(HF_val_events_max[ival, :],
                                               LFint2HF_rec_events_max[ival, :])

    # Plot data exclusive Aflood
    plotelements = \
        np.where(
            (LFint2HF_InunDetect == 0) | (LFint2HF_InunDetect == 1) | (LFint2HF_InunDetect == 2))[
            0]

    index_subplot = ival + 1
    ax1 = plt.subplot(2, 2, index_subplot)
    ax1.title.set_text('Event %i' % val_events[ival])
    hb = dfs_HF.plot(LFint2HF_InunDetect[plotelements], elements=plotelements, plot_type='patch',
                     show_mesh=False, cmap=color_map, vmin=-0.5, vmax=2.5, ax=ax1)
    # remove the x and y ticks
    ax1.axes.xaxis.set_visible(False)
    ax1.axes.yaxis.set_visible(False)

    # remove colorbar
    cbar = hb.collections[0].colorbar
    cbar.remove()



fig.tight_layout()

detected_patch = mpatches.Patch(color='dodgerblue', label='Detected')
FalseAlarm_patch = mpatches.Patch(color='yellow', label='False Alarm')
Misses_patch = mpatches.Patch(color='red', label='Miss')
leg = ax1.legend(title='Legend', handles=[detected_patch, FalseAlarm_patch, Misses_patch], loc='center left', bbox_to_anchor=(1.35, 0.5), frameon=False)
leg._legend_box.align = "left"

plt.savefig('Managed_Data/Classification_Figures/Detectec_falsealarm_misses.png',
            bbox_inches="tight", dpi=300)
plt.show()

#%% Plot the inundation for the LF, HF and LSG model at 3 different timesteps of event 1 for comparison
# Load Mike21 data for plotting
dfs_LF = Dfsu("Raw_Data/LF/Chow_LF_20110701_20111015.dfsu")  # dfsu object


# Timesteps to plot for comparison
plot_times_str = ['15/12/2010', '15/02/2011', '15/04/2011']
plot_times = pd.to_datetime(plot_times_str, format='%d/%m/%Y')

fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15*cm, 11*cm))
color_map = matplotlib.colors.ListedColormap(['dodgerblue'])

for t_idx in range(len(plot_times)):
    # Find index of closest timestep to the timestep for plotting
    t_nearest = np.argmin(abs(plot_times[t_idx] - Time[0]))

    #LF model
    plotelements = np.where(LF_events_bin[0][t_nearest] == 1)[0]  # Elements to plot
    #Plot data
    hb = dfs_LF.plot(LF_events_bin[0][t_nearest][plotelements], elements=plotelements, plot_type='patch',
                     show_mesh=False, cmap=color_map, vmin=0.5, vmax=1.5, ax=axs[t_idx, 0])
    # remove the x and y ticks
    axs[t_idx, 0].axes.xaxis.set_ticks([])
    axs[t_idx, 0].axes.yaxis.set_ticks([])
    # remove colorbar
    cbar = hb.collections[0].colorbar
    cbar.remove()

    # Add inundated area to figure
    A_LF = round(np.sum(LF_events_bin[0][t_nearest] * LF_Area) / 1e6, 1)
    axs[t_idx, 0].add_artist(AnchoredText(str(A_LF) + " km²", loc=3, borderpad=0.1, pad=0.2, frameon=False, prop=dict(fontsize=9)))

    #LSG model
    plotelements = np.where(LFint2HF_recon_events[0][t_nearest] == 1)[0]  # Elements to plot
    #Plot data
    hb = dfs_HF.plot(LFint2HF_recon_events[0][t_nearest][plotelements], elements=plotelements, plot_type='patch',
                     show_mesh=False, cmap=color_map, vmin=0.5, vmax=1.5, ax=axs[t_idx, 1])
    # remove the x and y ticks
    axs[t_idx, 1].axes.xaxis.set_ticks([])
    axs[t_idx, 1].axes.yaxis.set_ticks([])
    # remove colorbar
    cbar = hb.collections[0].colorbar
    cbar.remove()

    # Add inundated area to figure
    A_LSG = round(np.sum(LFint2HF_recon_events[0][t_nearest] * Area) / 1e6, 1)
    axs[t_idx, 1].add_artist(AnchoredText(str(A_LSG) + " km²", loc=3, borderpad=0.1, pad=0.2, frameon=False, prop=dict(fontsize=9)))

    #HF model
    plotelements = np.where(HF_events_bin[0][t_nearest] == 1)[0]  # Elements to plot
    #Plot data
    hb = dfs_HF.plot(HF_events_bin[0][t_nearest][plotelements], elements=plotelements, plot_type='patch',
                     show_mesh=False, cmap=color_map, vmin=0.5, vmax=1.5, ax=axs[t_idx, 2])
    # remove the x and y ticks
    axs[t_idx, 2].axes.xaxis.set_ticks([])
    axs[t_idx, 2].axes.yaxis.set_ticks([])
    # remove colorbar
    cbar = hb.collections[0].colorbar
    cbar.remove()

    # Add inundated area to figure
    A_HF = round(np.sum(HF_events_bin[0][t_nearest] * Area) / 1e6, 1)
    axs[t_idx, 2].add_artist(AnchoredText(str(A_HF) + " km²", loc=3, borderpad=0.1, pad=0.2, frameon=False, prop=dict(fontsize=9)))

    # Insert title
    if t_idx == 0:
        axs[t_idx, 0].title.set_text('Low-fidelity model')
        axs[t_idx, 1].title.set_text('LSG model')
        axs[t_idx, 2].title.set_text('High-fidelity model')

    #Scalebar
    if t_idx == 2:
        scalebar = ScaleBar(1, "m", length_fraction=0.25, location=2, width_fraction=0.025,
                            font_properties=dict(size=9), sep=2, frameon=False)  # 5 km scale
        axs[t_idx, 2].add_artist(scalebar)

    # Insert text showing the plotted time
    axs[t_idx, 0].set_ylabel(plot_times_str[t_idx], rotation=0, fontsize=10)
    axs[t_idx, 0].yaxis.set_label_coords(-.3, .5)

fig.tight_layout()
plt.savefig('Managed_Data/Classification_Figures/Inundation_evolution_event1.png',
            bbox_inches="tight", dpi=300)
plt.show()

